{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "train_path = np.load('/BrainSeg/data/patches_1024/train.npy')\n",
    "val_path = np.load('/BrainSeg/data/patches_1024/val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample function utilizing tf.keras.layers.Conv2D, size=4, strides=2, default set to be 2x resolution\n",
    "# based on reference http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/\n",
    "# upsample function, the size is determined by factor of images, strides is 2 * factor - factor % 2\n",
    "def upsample(filters, size=4, strides=2, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=strides,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn_model(classes=3, drop_out_rate=0.2, bn=True):\n",
    "    # use dropout and bacth normalization to prevent overfitting and help to do quick convergence\n",
    "    # activation layer is added to incorporate non-linearity\n",
    "    \n",
    "    # input layer has variable length in width and height, tested on both 512, 1024\n",
    "    input_imgs = tf.keras.layers.Input(shape=(None, None, 3))\n",
    "    \n",
    "    # All the kernel size, filter, stride are based on comparson paper, maxpooling layer is based on original FCN paper\n",
    "    \n",
    "    # First conv layer + max pooling\n",
    "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='same')(input_imgs)\n",
    "    x = tf.keras.layers.Dropout(drop_out_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x, training=bn)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "    \n",
    "    # Second conv layer + max pooling\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=1, padding='same')(pool1)\n",
    "    x = tf.keras.layers.Dropout(drop_out_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x, training=bn)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "    \n",
    "    # Third conv layer + max pooling\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(pool2)\n",
    "    x = tf.keras.layers.Dropout(drop_out_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x, training=bn)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    pool3 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "    \n",
    "    # Forth conv layer + max pooling\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\")(pool3)\n",
    "    x = tf.keras.layers.Dropout(drop_out_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x, training=bn)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    pool4 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)(x)  \n",
    "    \n",
    "    # Fifth conv layer + max pooling\n",
    "    x = tf.keras.layers.Conv2D(filters=1024, kernel_size=11, strides=1, padding=\"same\")(pool4)\n",
    "    x = tf.keras.layers.Dropout(drop_out_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x, training=bn)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    pool5 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "    \n",
    "    # build the fully connected layer using 1*1 convolutional layer\n",
    "    x = tf.keras.layers.Conv2D(filters=512, kernel_size=1, strides=1, padding=\"same\")(pool5)\n",
    "    x = tf.keras.layers.Dropout(drop_out_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x, training=bn)\n",
    "    conv6 = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=classes, kernel_size=1, strides=1, padding=\"same\")(conv6)\n",
    "    x = tf.keras.layers.Dropout(drop_out_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x, training=bn)\n",
    "    conv7 = tf.keras.layers.Activation('sigmoid')(x)\n",
    "\n",
    "    # upsampling conv7 to 4x times and upsample pool4 to 2x times\n",
    "    up_conv7 = upsample(filters=classes, size=8, strides=4)(conv7)\n",
    "    up_pool4 = upsample(filters=64)(pool4)\n",
    "    \n",
    "    # Concatenate two resolutions\n",
    "    fuse_1 = tf.keras.layers.Concatenate()([up_conv7, up_pool4])\n",
    "    fuse_2 = tf.keras.layers.Concatenate()([fuse_1, pool3])\n",
    "    \n",
    "    prob = upsample(filters=classes, size=16, strides=8)(fuse_2)\n",
    "    model = tf.keras.Model(inputs=input_imgs, outputs=prob)\n",
    "    \n",
    "    print(model.summary())\n",
    "    print(\"FCN model building completes\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class structure, combine with numpy array color normalization\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from typing import Tuple\n",
    "from nptyping import NDArray\n",
    "from PIL import Image\n",
    "class BrainSegSequence(Sequence):\n",
    "    def __init__(self, image_paths: NDArray[str],\n",
    "            mask_paths: NDArray[str], batch_size: int):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths  = mask_paths\n",
    "        self.batch_size  = batch_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[NDArray[np.uint8], NDArray[np.uint8]]:\n",
    "        batch_x = self.image_paths[idx * self.batch_size : \n",
    "                (idx+1) * self.batch_size]\n",
    "        batch_y = self.mask_paths[idx * self.batch_size : \n",
    "                (idx+1) * self.batch_size]\n",
    "        return np.array([np.array(Image.open(p)) for p in batch_x])/255.0, \\\n",
    "                np.array([np.array(Image.open(p)) for p in batch_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the training and val dataset with batchsize 16\n",
    "BATCH_SIZE = 16\n",
    "train_dataset = BrainSegSequence(train_path[:,0], train_path[:,1], BATCH_SIZE)\n",
    "val_dataset = BrainSegSequence(val_path[:,0], val_path[:,1], BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_filter(data_path, slide_index):\n",
    "    filter_ls = []\n",
    "    for ele in data_path:\n",
    "        if ele[0].split('/')[-2] == slide_index:\n",
    "            filter_ls.append(ele)\n",
    "        elif ele[0].split('/')[-2] == slide_index+'17-24':\n",
    "            filter_ls.append(ele)\n",
    "    dataset = BrainSegSequence(np.array(filter_ls)[:,0], np.array(filter_ls)[:,1], BATCH_SIZE)\n",
    "    return slide_index, len(filter_ls), dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_name_ls(address):\n",
    "    # Input the address of the text file, read in the train file names, and val file names\n",
    "    # return the length of train files, length of val files and train, val file lists\n",
    "    f = open(address, \"r\")\n",
    "    train_signal=False\n",
    "    val_signal = False\n",
    "    train_name_ls = [] \n",
    "    val_name_ls = []\n",
    "    for line in f:\n",
    "        if len(line.split('/')) > 1:\n",
    "            if line.split('/')[4][:2] == 'NA' and train_signal:\n",
    "                train_name_ls.append(line.split('/')[4][:12])\n",
    "            elif line.split('/')[4][:2] == 'NA' and val_signal:\n",
    "                val_name_ls.append(line.split('/')[4][:12])\n",
    "        else:\n",
    "            if line.split('/')[0] == '\\tTrain: \\n':\n",
    "                train_signal = True\n",
    "                val_signal = False\n",
    "            elif line.split('/')[0] == '\\tVal: \\n':\n",
    "                val_signal = True\n",
    "                train_signal = False\n",
    "    f.close()\n",
    "    return len(train_name_ls), len(val_name_ls), train_name_ls, val_name_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, train_file_ls, val_file_ls = slide_name_ls('/BrainSeg/data/patches_1024/dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_slides(file_ls, path):\n",
    "    # define and compile model again, set the batchnormalization training to be false to use accumulated statistics\n",
    "    model = fcn_model(classes=3, bn=False)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=[ tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    # return the latest weights in the folder\n",
    "    latest = tf.train.latest_checkpoint('/BrainSeg/baseline_code/5_3_saved_weights/')\n",
    "    # Load the previously saved weights\n",
    "    model.load_weights(latest)\n",
    "    for file in file_ls:\n",
    "        slide_index, data_size, cur_dataset = slide_filter(path, file)\n",
    "        # evaluate the model performance on this specific slide\n",
    "        loss, acc = model.evaluate(cur_dataset)\n",
    "        print(\"The {0} slide {1}: Accuracy: \".format('Validation', slide_index)+\"{:5.2f}%\".format(acc*100)+\", Losses: {:5.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_26 (InputLayer)           [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, None, None, 1 1216        input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_175 (Dropout)           (None, None, None, 1 0           conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_250 (BatchN (None, None, None, 1 64          dropout_175[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, None, None, 1 0           batch_normalization_250[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_125 (MaxPooling2D (None, None, None, 1 0           activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, None, None, 3 12832       max_pooling2d_125[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_176 (Dropout)           (None, None, None, 3 0           conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_251 (BatchN (None, None, None, 3 128         dropout_176[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, None, None, 3 0           batch_normalization_251[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_126 (MaxPooling2D (None, None, None, 3 0           activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, None, None, 6 18496       max_pooling2d_126[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_177 (Dropout)           (None, None, None, 6 0           conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_252 (BatchN (None, None, None, 6 256         dropout_177[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, None, None, 6 0           batch_normalization_252[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_127 (MaxPooling2D (None, None, None, 6 0           activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, None, None, 6 36928       max_pooling2d_127[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_178 (Dropout)           (None, None, None, 6 0           conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_253 (BatchN (None, None, None, 6 256         dropout_178[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, None, None, 6 0           batch_normalization_253[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_128 (MaxPooling2D (None, None, None, 6 0           activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, None, None, 1 7930880     max_pooling2d_128[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_179 (Dropout)           (None, None, None, 1 0           conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_254 (BatchN (None, None, None, 1 4096        dropout_179[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, None, None, 1 0           batch_normalization_254[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_129 (MaxPooling2D (None, None, None, 1 0           activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, None, None, 5 524800      max_pooling2d_129[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_180 (Dropout)           (None, None, None, 5 0           conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_255 (BatchN (None, None, None, 5 2048        dropout_180[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, None, None, 5 0           batch_normalization_255[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, None, None, 3 1539        activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_181 (Dropout)           (None, None, None, 3 0           conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_256 (BatchN (None, None, None, 3 12          dropout_181[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, None, None, 3 0           batch_normalization_256[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_75 (Sequential)      (None, None, None, 3 588         activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_76 (Sequential)      (None, None, None, 6 65792       max_pooling2d_128[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, None, None, 6 0           sequential_75[0][0]              \n",
      "                                                                 sequential_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, None, None, 1 0           concatenate_50[0][0]             \n",
      "                                                                 max_pooling2d_127[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "sequential_77 (Sequential)      (None, None, None, 3 100620      concatenate_51[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 8,700,551\n",
      "Trainable params: 8,696,981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-trainable params: 3,570\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "FCN model building completes\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f5826a21510> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f5826a21510> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "207/207 [==============================] - 209s 1s/step - loss: 0.3356 - sparse_categorical_accuracy: 0.9083\n",
      "The Validation slide NA4077-02_AB: Accuracy: 90.83%, Losses: 0.3356\n",
      "183/183 [==============================] - 191s 1s/step - loss: 0.0983 - sparse_categorical_accuracy: 0.9693\n",
      "The Validation slide NA4092-02_AB: Accuracy: 96.93%, Losses: 0.0983\n",
      "151/151 [==============================] - 149s 988ms/step - loss: 0.2129 - sparse_categorical_accuracy: 0.9427\n",
      "The Validation slide NA4107-02_AB: Accuracy: 94.27%, Losses: 0.2129\n",
      "150/150 [==============================] - 157s 1s/step - loss: 0.3400 - sparse_categorical_accuracy: 0.8890\n",
      "The Validation slide NA4463-02_AB: Accuracy: 88.90%, Losses: 0.3400\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.1525 - sparse_categorical_accuracy: 0.9507\n",
      "The Validation slide NA4691-02_AB: Accuracy: 95.07%, Losses: 0.1525\n",
      "141/141 [==============================] - 145s 1s/step - loss: 0.2397 - sparse_categorical_accuracy: 0.9218\n",
      "The Validation slide NA4695-02_AB: Accuracy: 92.18%, Losses: 0.2397\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-f17b5298975a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_slides\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_file_ls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-222-329493338d6b>\u001b[0m in \u001b[0;36mevaluate_slides\u001b[0;34m(file_ls, path)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_ls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mslide_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslide_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# evaluate the model performance on this specific slide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-219-5cc8d7c8f79a>\u001b[0m in \u001b[0;36mslide_filter\u001b[0;34m(data_path, slide_index)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mele\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mslide_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mfilter_ls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBrainSegSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mslide_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "evaluate_slides(val_file_ls, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 1 1216        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, None, None, 1 0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, None, 1 64          dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, None, 1 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, None, None, 1 0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 3 12832       max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, None, None, 3 0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 3 128         dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, None, 3 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, None, None, 3 0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, None, None, 6 18496       max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, None, None, 6 0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 6 256         dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, None, 6 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D) (None, None, None, 6 0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, None, None, 6 36928       max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, None, None, 6 0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 6 256         dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, None, 6 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, None, None, 6 0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, None, None, 1 7930880     max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, None, None, 1 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 1 4096        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, None, 1 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, None, None, 1 0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, None, None, 5 524800      max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, None, None, 5 0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 5 2048        dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, None, 5 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, None, None, 3 1539        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, None, None, 3 0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 3 12          dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, None, 3 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, None, None, 3 588         activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, None, None, 6 65792       max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, None, None, 6 0           sequential_9[0][0]               \n",
      "                                                                 sequential_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, None, None, 1 0           concatenate_6[0][0]              \n",
      "                                                                 max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, None, None, 3 100620      concatenate_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,700,551\n",
      "Trainable params: 8,696,981\n",
      "Non-trainable params: 3,570\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "FCN model building completes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f75c41e0840> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f75c41e0840> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "156/156 [==============================] - 159s 1s/step - loss: 0.2859 - sparse_categorical_accuracy: 0.9272\n",
      "The Validation slide NA3777-02_AB: Accuracy: 92.72%, Losses: 0.2859\n",
      "152/152 [==============================] - 155s 1s/step - loss: 0.1180 - sparse_categorical_accuracy: 0.9619\n",
      "The Validation slide NA4160-02_AB: Accuracy: 96.19%, Losses: 0.1180\n",
      "135/135 [==============================] - 141s 1s/step - loss: 0.1892 - sparse_categorical_accuracy: 0.9389\n",
      "The Validation slide NA4195-02_AB: Accuracy: 93.89%, Losses: 0.1892\n",
      "176/176 [==============================] - 190s 1s/step - loss: 0.3146 - sparse_categorical_accuracy: 0.9018\n",
      "The Validation slide NA4256-02_AB: Accuracy: 90.18%, Losses: 0.3146\n",
      "154/154 [==============================] - 162s 1s/step - loss: 0.2583 - sparse_categorical_accuracy: 0.9244\n",
      "The Validation slide NA4299-02_AB: Accuracy: 92.44%, Losses: 0.2583\n",
      "144/144 [==============================] - 153s 1s/step - loss: 0.1814 - sparse_categorical_accuracy: 0.9426\n",
      "The Validation slide NA4391-02_AB: Accuracy: 94.26%, Losses: 0.1814\n",
      "168/168 [==============================] - 169s 1s/step - loss: 0.1140 - sparse_categorical_accuracy: 0.9673\n",
      "The Validation slide NA4450-02_AB: Accuracy: 96.73%, Losses: 0.1140\n",
      "209/209 [==============================] - 207s 990ms/step - loss: 0.2210 - sparse_categorical_accuracy: 0.9359\n",
      "The Validation slide NA4471-02_AB: Accuracy: 93.59%, Losses: 0.2210\n",
      "197/197 [==============================] - 183s 926ms/step - loss: 0.2318 - sparse_categorical_accuracy: 0.9302\n",
      "The Validation slide NA4553-02_AB: Accuracy: 93.02%, Losses: 0.2318\n",
      "121/121 [==============================] - 128s 1s/step - loss: 0.2343 - sparse_categorical_accuracy: 0.9246\n",
      "The Validation slide NA4626-02_AB: Accuracy: 92.46%, Losses: 0.2343\n",
      "111/111 [==============================] - 112s 1s/step - loss: 0.1018 - sparse_categorical_accuracy: 0.9678\n",
      "The Validation slide NA4672-02_AB: Accuracy: 96.78%, Losses: 0.1018\n",
      "150/150 [==============================] - 154s 1s/step - loss: 0.1492 - sparse_categorical_accuracy: 0.9526\n",
      "The Validation slide NA4675-02_AB: Accuracy: 95.26%, Losses: 0.1492\n",
      "163/163 [==============================] - 165s 1s/step - loss: 0.2118 - sparse_categorical_accuracy: 0.9305\n",
      "The Validation slide NA4894-02_AB: Accuracy: 93.05%, Losses: 0.2118\n",
      "109/109 [==============================] - 104s 959ms/step - loss: 0.1316 - sparse_categorical_accuracy: 0.9540\n",
      "The Validation slide NA4944-02_AB: Accuracy: 95.40%, Losses: 0.1316\n",
      "145/145 [==============================] - 138s 952ms/step - loss: 0.3519 - sparse_categorical_accuracy: 0.8879\n",
      "The Validation slide NA4971-02_AB: Accuracy: 88.79%, Losses: 0.3519\n",
      "86/86 [==============================] - 80s 936ms/step - loss: 0.2620 - sparse_categorical_accuracy: 0.9258\n",
      "The Validation slide NA4992-02_AB: Accuracy: 92.58%, Losses: 0.2620\n",
      "352/352 [==============================] - 381s 1s/step - loss: 0.2981 - sparse_categorical_accuracy: 0.9113\n",
      "The Validation slide NA5010-02_AB: Accuracy: 91.13%, Losses: 0.2981\n",
      "143/143 [==============================] - 142s 990ms/step - loss: 0.1777 - sparse_categorical_accuracy: 0.9423\n",
      "The Validation slide NA5029-02_AB: Accuracy: 94.23%, Losses: 0.1777\n"
     ]
    }
   ],
   "source": [
    "evaluate_slides(train_file_ls, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 1 1216        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, None, None, 1 0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, None, 1 64          dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, None, 1 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, None, None, 1 0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 3 12832       max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, None, None, 3 0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 3 128         dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, None, 3 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, None, None, 3 0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, None, None, 6 18496       max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, None, None, 6 0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 6 256         dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, None, 6 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D) (None, None, None, 6 0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, None, None, 6 36928       max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, None, None, 6 0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 6 256         dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, None, 6 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, None, None, 6 0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, None, None, 1 7930880     max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, None, None, 1 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 1 4096        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, None, 1 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, None, None, 1 0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, None, None, 5 524800      max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, None, None, 5 0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 5 2048        dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, None, 5 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, None, None, 3 1539        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, None, None, 3 0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 3 12          dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, None, 3 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, None, None, 3 588         activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, None, None, 6 65792       max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, None, None, 6 0           sequential_9[0][0]               \n",
      "                                                                 sequential_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, None, None, 1 0           concatenate_6[0][0]              \n",
      "                                                                 max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, None, None, 3 100620      concatenate_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,700,551\n",
      "Trainable params: 8,696,981\n",
      "Non-trainable params: 3,570\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "FCN model building completes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fa5a0173a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fa5a0173a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "103/103 [==============================] - 96s 928ms/step - loss: 0.1048 - sparse_categorical_accuracy: 0.9640\n",
      "The Validation slide NA4967-02_AB: Accuracy: 96.40%, Losses: 0.1048\n",
      "115/115 [==============================] - 101s 879ms/step - loss: 0.3090 - sparse_categorical_accuracy: 0.8960\n",
      "The Validation slide NA4972-02_AB: Accuracy: 89.60%, Losses: 0.3090\n",
      "100/100 [==============================] - 100s 1s/step - loss: 0.4811 - sparse_categorical_accuracy: 0.8497\n",
      "The Validation slide NA4993-02_AB: Accuracy: 84.97%, Losses: 0.4811\n"
     ]
    }
   ],
   "source": [
    "evaluate_slides(val_file_ls[7:], val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
